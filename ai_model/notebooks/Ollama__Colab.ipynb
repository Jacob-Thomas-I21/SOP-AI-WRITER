{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pharmaceutical_ollama_setup"
      },
      "source": [
        "\"\"\"\n",
        "Author: Jacob Thomas Joshy\n",
        "Purpose: Complete environment configuration for fine-tuning Ollama models\n",
        "         specifically for pharmaceutical Standard Operating Procedure (SOP) generation\n",
        "         \n",
        "This notebook provides a comprehensive setup for:\n",
        "- GPU-accelerated Ollama installation and configuration  \n",
        "- Pharmaceutical-specific dataset preparation utilities\n",
        "- FDA compliance validation framework integration\n",
        "- Model evaluation and performance metrics\n",
        "\n",
        "Requirements:\n",
        "- Google Colab Pro recommended for GPU access (T4/V100)\n",
        "- Minimum 12GB RAM for model fine-tuning\n",
        "- Stable internet connection for model downloads\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "system_requirements"
      },
      "source": [
        "## 1. System Requirements & GPU Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "check_gpu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed25e680-b121-4458-b2ad-5d5a9e5c1203"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Aug 27 10:22:49 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "CUDA available: True\n",
            "GPU count: 1\n",
            "Current GPU: Tesla T4\n",
            "GPU memory: 14.7 GB\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability and specifications\n",
        "!nvidia-smi\n",
        "import torch\n",
        "import os\n",
        "import subprocess\n",
        "import json\n",
        "\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
        "    print(f\"Current GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "else:\n",
        "    print(\"Warning: GPU not available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_dependencies"
      },
      "source": [
        "## 2. Install Dependencies & Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "install_ollama",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "782259ed-dade-4851-ecdd-84f06de3e33a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.2/55.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/61.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.4/69.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pubchempy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "All dependencies installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install Ollama and required dependencies\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "# Install Python dependencies for pharmaceutical data processing\n",
        "!pip install -q transformers datasets accelerate bitsandbytes\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q requests beautifulsoup4 pandas numpy\n",
        "!pip install -q matplotlib seaborn plotly\n",
        "!pip install -q scikit-learn nltk spacy\n",
        "!pip install -q jupyter-client ipywidgets\n",
        "\n",
        "# Install pharmaceutical-specific libraries\n",
        "!pip install -q biopython  # Biological data processing\n",
        "!pip install -q chembl_webresource_client  # Chemical database access\n",
        "!pip install -q pubchempy  # PubChem database access\n",
        "print(\"All dependencies installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "start_ollama_service",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35980db4-01f0-49ca-cb11-b5d1a75be6df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ollama service is running\n",
            "Current models:\n",
            "NAME    ID    SIZE    MODIFIED \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize Ollama background service\n",
        "import subprocess\n",
        "import time\n",
        "import threading\n",
        "\n",
        "def launch_ollama():\n",
        "    \"\"\"Launch ollama server process in background\"\"\"\n",
        "    try:\n",
        "        proc = subprocess.run(['ollama', 'serve'], check=True, capture_output=True)\n",
        "    except subprocess.CalledProcessError as err:\n",
        "        print(f\"Service launch failed: {err}\")\n",
        "\n",
        "# Create background thread for service\n",
        "bg_thread = threading.Thread(target=launch_ollama, daemon=True)\n",
        "bg_thread.start()\n",
        "\n",
        "# Allow startup time\n",
        "time.sleep(5)\n",
        "\n",
        "# Check if service is responding\n",
        "try:\n",
        "    check_result = subprocess.run(['ollama', 'list'],\n",
        "                                 capture_output=True,\n",
        "                                 text=True,\n",
        "                                 timeout=10)\n",
        "\n",
        "    if check_result.returncode == 0:\n",
        "        print(\"Ollama service is running\")\n",
        "        print(\"Current models:\")\n",
        "        print(check_result.stdout)\n",
        "    else:\n",
        "        print(\"Service may still be initializing\")\n",
        "\n",
        "except subprocess.TimeoutExpired:\n",
        "    print(\"Service check timed out - continuing anyway\")\n",
        "except Exception as error:\n",
        "    print(f\"Service verification failed: {error}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download_models"
      },
      "source": [
        "## 3. Download Base Models for Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "download_models_fixed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "944a8a1d-2fd7-414d-f0c2-4d596304687f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available models for pharmaceutical SOP generation:\n",
            "1. llama2:7b-chat (4GB approx)\n",
            "   Llama 2 7B for general pharmaceutical text\n",
            "\n",
            "2. llama2:13b-chat (7GB approx)\n",
            "   Llama 2 13B higher quality but memory intensive\n",
            "\n",
            "3. mistral:7b-instruct (4GB approx)\n",
            "   Mistral 7B optimized for technical documentation\n",
            "\n",
            "Downloading model: mistral:7b-instruct\n",
            "Download time: 5-10 minutes depending on connection\n",
            "Download completed: mistral:7b-instruct\n",
            "\n",
            "Installed models:\n",
            "NAME                   ID              SIZE      MODIFIED               \n",
            "mistral:7b-instruct    6577803aa9a0    4.4 GB    Less than a second ago    \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Model selection and download for pharmaceutical text generation\n",
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Available model options with specifications\n",
        "available_models = [\n",
        "    {\n",
        "        'name': 'llama2:7b-chat',\n",
        "        'description': 'Llama 2 7B for general pharmaceutical text',\n",
        "        'size': '4GB approx'\n",
        "    },\n",
        "    {\n",
        "        'name': 'llama2:13b-chat',\n",
        "        'description': 'Llama 2 13B higher quality but memory intensive',\n",
        "        'size': '7GB approx'\n",
        "    },\n",
        "    {\n",
        "        'name': 'mistral:7b-instruct',\n",
        "        'description': 'Mistral 7B optimized for technical documentation',\n",
        "        'size': '4GB approx'\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"Available models for pharmaceutical SOP generation:\")\n",
        "for idx, model_info in enumerate(available_models, 1):\n",
        "    print(f\"{idx}. {model_info['name']} ({model_info['size']})\")\n",
        "    print(f\"   {model_info['description']}\")\n",
        "    print()\n",
        "\n",
        "# Select model for download\n",
        "target_model = 'mistral:7b-instruct'\n",
        "print(f\"Downloading model: {target_model}\")\n",
        "print(\"Download time: 5-10 minutes depending on connection\")\n",
        "\n",
        "# Use os.system for progress visibility in notebook environment\n",
        "download_status = os.system(f'ollama pull {target_model}')\n",
        "\n",
        "if download_status == 0:\n",
        "    print(f\"Download completed: {target_model}\")\n",
        "else:\n",
        "    print(f\"Download failed with exit code: {download_status}\")\n",
        "    print(\"Manual download may be required\")\n",
        "\n",
        "# Verify available models\n",
        "print(\"\\nInstalled models:\")\n",
        "try:\n",
        "    model_list = subprocess.run(['ollama', 'list'], capture_output=True, text=True)\n",
        "    print(model_list.stdout)\n",
        "except Exception as err:\n",
        "    print(f\"Could not list models: {err}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngrok_setup"
      },
      "source": [
        "## 4. Setup Ngrok Authentication & Tunnel\n",
        "\n",
        "This section sets up ngrok tunneling to connect your Google Colab Ollama instance to your local backend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "setup_ngrok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4829a02c-3d26-4414-de0b-c20abf958a11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up ngrok tunnel authentication...\n",
            "\n",
            "Setup Instructions:\n",
            "1. Register at: https://dashboard.ngrok.com/signup\n",
            "2. Copy authtoken from: https://dashboard.ngrok.com/get-started/your-authtoken\n",
            "3. Create environment file:\n",
            "   !echo 'NGROK_AUTH_TOKEN=your_token_here' > /content/.env\n",
            "   Replace 'your_token_here' with actual token\n",
            "\n",
            "ERROR: NGROK_AUTH_TOKEN not found in environment file\n",
            "\n",
            "To fix:\n",
            "1. Create environment file with token:\n",
            "   !echo 'NGROK_AUTH_TOKEN=your_actual_token' > /content/.env\n",
            "2. Run this cell again\n",
            "\n",
            "Example .env content:\n",
            "   NGROK_AUTH_TOKEN=2abc123def456ghi789jkl0mno1pqr2_3StUvWxYz4AbCdEfGhIj\n"
          ]
        }
      ],
      "source": [
        "# Install tunnel dependencies\n",
        "import subprocess\n",
        "subprocess.check_call(['pip', 'install', 'pyngrok', 'python-dotenv'])\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from pyngrok import ngrok\n",
        "import time\n",
        "\n",
        "print(\"Setting up ngrok tunnel authentication...\")\n",
        "print(\"\\nSetup Instructions:\")\n",
        "print(\"1. Register at: https://dashboard.ngrok.com/signup\")\n",
        "print(\"2. Copy authtoken from: https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "print(\"3. Create environment file:\")\n",
        "print(\"   !echo 'NGROK_AUTH_TOKEN=your_token_here' > /content/.env\")\n",
        "print(\"   Replace 'your_token_here' with actual token\")\n",
        "\n",
        "# Load token from environment file\n",
        "load_dotenv('/content/.env')\n",
        "token = os.getenv('NGROK_AUTH_TOKEN')\n",
        "\n",
        "if not token:\n",
        "    print(\"\\nERROR: NGROK_AUTH_TOKEN not found in environment file\")\n",
        "    print(\"\\nTo fix:\")\n",
        "    print(\"1. Create environment file with token:\")\n",
        "    print(\"   !echo 'NGROK_AUTH_TOKEN=your_actual_token' > /content/.env\")\n",
        "    print(\"2. Run this cell again\")\n",
        "    print(\"\\nExample .env content:\")\n",
        "    print(\"   NGROK_AUTH_TOKEN=2abc123def456ghi789jkl0mno1pqr2_3StUvWxYz4AbCdEfGhIj\")\n",
        "\n",
        "else:\n",
        "    print(f\"Auth token found: {token[:8]}...\")\n",
        "\n",
        "    try:\n",
        "        ngrok.set_auth_token(token)\n",
        "        print(\"Ngrok authentication successful\")\n",
        "\n",
        "        # Create tunnel for Ollama service\n",
        "        print(\"\\nCreating tunnel to Ollama port 11434...\")\n",
        "        tunnel_url = ngrok.connect(11434)\n",
        "\n",
        "        print(f\"\\nOllama API accessible at: {tunnel_url}\")\n",
        "        print(f\"Add to your backend configuration:\")\n",
        "        print(f\"   OLLAMA_BASE_URL='{tunnel_url}'\")\n",
        "\n",
        "        # Test tunnel connectivity\n",
        "        print(f\"\\nTesting tunnel connection...\")\n",
        "        import requests\n",
        "        try:\n",
        "            response = requests.get(f\"{tunnel_url}/api/tags\", timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                print(\"Tunnel test successful - backend can connect\")\n",
        "            else:\n",
        "                print(f\"Tunnel created but service returned: {response.status_code}\")\n",
        "        except Exception as test_error:\n",
        "            print(f\"Tunnel test failed: {test_error}\")\n",
        "            print(\"This may be normal during Ollama startup\")\n",
        "\n",
        "        print(f\"\\nKeep this session running for tunnel access\")\n",
        "        print(f\"Active tunnel: {tunnel_url}\")\n",
        "\n",
        "    except Exception as setup_error:\n",
        "        print(f\"Ngrok setup failed: {setup_error}\")\n",
        "        print(\"\\nTroubleshooting:\")\n",
        "        print(\"- Check token validity\")\n",
        "        print(\"- Verify ngrok account status\")\n",
        "        print(\"- Try runtime restart\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "connection_instructions"
      },
      "source": [
        "## 5. Connect to Local Backend\n",
        "\n",
        "Instructions for connecting your local pharmaceutical SOP backend to this Colab Ollama instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "connection_instructions_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f33c0097-cae9-4bba-9222-f9f46ca005dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backend Connection Setup\n",
            "==============================\n",
            "\n",
            "Connection steps:\n",
            "1. Copy ngrok URL from above to your backend .env:\n",
            "   OLLAMA_BASE_URL='https://abc123.ngrok.io'\n",
            "\n",
            "2. Navigate to your project directory:\n",
            "   cd sop-author-pharmaceutical/backend\n",
            "\n",
            "3. Setup Python environment:\n",
            "   python -m venv venv\n",
            "   venv\\Scripts\\activate    # Windows\n",
            "   source venv/bin/activate  # Mac/Linux\n",
            "\n",
            "4. Install dependencies and start server:\n",
            "   pip install -r requirements.txt\n",
            "   uvicorn app.main:app --reload --port 9000\n",
            "\n",
            "5. Access points:\n",
            "   Frontend: http://localhost:5173/\n",
            "   API docs: http://localhost:9000/docs\n",
            "\n",
            "Note: Keep this Colab session active while using local app\n",
            "Session typically lasts 12 hours\n"
          ]
        }
      ],
      "source": [
        "# Connect Colab Ollama to local backend\n",
        "print(\"Backend Connection Setup\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "print(\"\\nConnection steps:\")\n",
        "print(\"1. Copy ngrok URL from above to your backend .env:\")\n",
        "print(\"   OLLAMA_BASE_URL='https://abc123.ngrok.io'\")\n",
        "\n",
        "print(\"\\n2. Navigate to your project directory:\")\n",
        "print(\"   cd sop-author-pharmaceutical/backend\")\n",
        "\n",
        "print(\"\\n3. Setup Python environment:\")\n",
        "print(\"   python -m venv venv\")\n",
        "print(\"   venv\\\\Scripts\\\\activate    # Windows\")\n",
        "print(\"   source venv/bin/activate  # Mac/Linux\")\n",
        "\n",
        "print(\"\\n4. Install dependencies and start server:\")\n",
        "print(\"   pip install -r requirements.txt\")\n",
        "print(\"   uvicorn app.main:app --reload --port 9000\")\n",
        "\n",
        "print(\"\\n5. Access points:\")\n",
        "print(\"   Frontend: http://localhost:5173/\")\n",
        "print(\"   API docs: http://localhost:9000/docs\")\n",
        "\n",
        "print(\"\\nNote: Keep this Colab session active while using local app\")\n",
        "print(\"Session typically lasts 12 hours\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_ollama"
      },
      "source": [
        "## 6. Test Ollama Generation\n",
        "\n",
        "Test pharmaceutical SOP generation with the installed model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "test_generation",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "596afe3f-6bd4-41d0-b587-da50ccec842b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing model generation...\n",
            "Model: mistral:7b-instruct\n",
            "Prompt: Create a brief SOP outline for equipment cleaning validation...\n",
            "\n",
            "Generation result:\n",
            "Title: Standard Operating Procedure (SOP) for Equipment Cleaning Validation in Pharmaceutical Manufacturing\n",
            "\n",
            "1. **Introduction**\n",
            "   - Purpose of the SOP\n",
            "   - References and applicable guidelines\n",
            "   - Scope and limitations\n",
            "\n",
            "2. **Definitions, Abbreviations, and Acronyms**\n",
            "   - Clarification of key ter...\n",
            "\n",
            "Status: Model working correctly\n",
            "Environment ready for pharmaceutical SOP generation\n"
          ]
        }
      ],
      "source": [
        "# Test Ollama model generation\n",
        "import subprocess\n",
        "\n",
        "def run_model_test(model_name, test_prompt):\n",
        "    \"\"\"Simple test of model generation\"\"\"\n",
        "    try:\n",
        "        cmd = ['ollama', 'run', model_name]\n",
        "        result = subprocess.run(cmd, input=test_prompt,\n",
        "                              capture_output=True, text=True, timeout=30)\n",
        "        if result.returncode == 0:\n",
        "            return result.stdout.strip()\n",
        "        else:\n",
        "            return \"Generation failed\"\n",
        "    except subprocess.TimeoutExpired:\n",
        "        return \"Test timed out\"\n",
        "    except Exception:\n",
        "        return \"Test error\"\n",
        "\n",
        "# Test pharmaceutical SOP generation\n",
        "sample_prompt = \"Create a brief SOP outline for equipment cleaning validation in pharmaceutical manufacturing.\"\n",
        "\n",
        "print(\"Testing model generation...\")\n",
        "print(f\"Model: mistral:7b-instruct\")\n",
        "print(f\"Prompt: {sample_prompt[:60]}...\")\n",
        "\n",
        "test_output = run_model_test('mistral:7b-instruct', sample_prompt)\n",
        "\n",
        "print(\"\\nGeneration result:\")\n",
        "if len(test_output) > 300:\n",
        "    print(test_output[:300] + \"...\")\n",
        "else:\n",
        "    print(test_output)\n",
        "\n",
        "# Simple status check\n",
        "if \"failed\" in test_output.lower() or \"error\" in test_output.lower():\n",
        "    print(\"\\nStatus: Model test failed - check Ollama installation\")\n",
        "else:\n",
        "    print(\"\\nStatus: Model working correctly\")\n",
        "    print(\"Environment ready for pharmaceutical SOP generation\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}